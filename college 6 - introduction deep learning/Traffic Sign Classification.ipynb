{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Sign Classification\n",
    "\n",
    "bron: https://www.kaggle.com/niranjankumarc/traffic-sign-classification-keras-tensorflow-2-0/data\n",
    "\n",
    "\n",
    "![traffic_sign_classification_dataset.jpg](images/traffic_sign_classification_dataset.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "#import tensorflow \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D, Activation, Flatten, Dropout\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check tensorflow version\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a custom class to train the model.\n",
    "\n",
    "class TrafficSignNet:\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, classes):\n",
    "        model = Sequential()\n",
    "        inputShape = (height, width, depth)\n",
    "        chanDim = -1\n",
    "        \n",
    "        #layer: Conv -> RELU -> BN -> POOL\n",
    "        model.add(Conv2D(8, (5,5), padding = \"same\", input_shape = inputShape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis = chanDim))\n",
    "        model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "        \n",
    "        # first set of (CONV => RELU => CONV => RELU) * 2 => POOL\n",
    "        model.add(Conv2D(16, (3,3), padding = \"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis = chanDim))\n",
    "        model.add(Conv2D(16, (3,3), padding = \"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis = chanDim))\n",
    "        model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "        \n",
    "        # second set of (CONV => RELU => CONV => RELU) * 2 => POOL\n",
    "        model.add(Conv2D(32, (3,3), padding = \"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis = chanDim))\n",
    "        model.add(Conv2D(32, (3,3), padding = \"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis = chanDim))\n",
    "        model.add(Conv2D(32, (3,3), padding = \"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis = chanDim))\n",
    "        model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "        \n",
    "        #FC layers\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.7))\n",
    "        \n",
    "        #FC layers\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        #softmax\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a class object\n",
    "\n",
    "objTSN = TrafficSignNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary packages\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn\")\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from skimage import transform, exposure, io\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can automatically improve image contrast by applying an algorithm called Contrast Limited Adaptive Histogram Equalization (CLAHE), the implementation of which can be found in the scikit-image library.\n",
    "\n",
    "Using CLAHE we can improve the contrast of our traffic sign images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to load data from disk\n",
    "\n",
    "def load_split(basePath, csvPath):\n",
    "    #intialize the list of data and labels\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    # load the contents of the CSV file, remove the first line (since it contains the CSV header)\n",
    "    rows = open(csvPath).read().strip().split(\"\\n\")[1:]\n",
    "    random.shuffle(rows)\n",
    "    \n",
    "    #loop over the rows of csv file\n",
    "    for (i, row) in enumerate(rows):\n",
    "        #check to see if we should show a status update\n",
    "        if i > 0 and i % 4000 == 0:\n",
    "            print(\"[INFO] processed {} total images\".format(i))\n",
    "            \n",
    "        # split the row into components and then grab the class ID and image path\n",
    "        (label, imagePath) = row.strip().split(\",\")[-2:]\n",
    "        \n",
    "        # derive the full path to the image file and load it\n",
    "        imagePath = os.path.sep.join([basePath, imagePath])\n",
    "        #print(imagePath)\n",
    "        image = io.imread(imagePath)\n",
    "        \n",
    "        #resize the image to be 32x32 pixels, ignoring aspect ratio, and perform CLAHE.\n",
    "        image = transform.resize(image, (32, 32))\n",
    "        image = exposure.equalize_adapthist(image, clip_limit = 0.1)\n",
    "        \n",
    "        #update the list of data and labels, respectively\n",
    "        data.append(image)\n",
    "        labels.append(int(label))\n",
    "        \n",
    "    #convert the data and labels into numpy arrays\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    #return a tuple of the data and labels\n",
    "    return (data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the hyperparameters: \n",
    "#      initial learning rate, batch size, and number of epochs to train for\n",
    "\n",
    "INIT_LR = 1e-3\n",
    "BS = 64\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "#load the label names\n",
    "\n",
    "labelNames = ['20 km/h', '30 km/h', '50 km/h', '60 km/h', '70 km/h', '80 km/h', '80 km/h end', '100 km/h', '120 km/h', 'No overtaking',\n",
    "               'No overtaking for tracks', 'Crossroad with secondary way', 'Main road', 'Give way', 'Stop', 'Road up', 'Road up for track', 'Brock',\n",
    "               'Other dangerous', 'Turn left', 'Turn right', 'Winding road', 'Hollow road', 'Slippery road', 'Narrowing road', 'Roadwork', 'Traffic light',\n",
    "               'Pedestrian', 'Children', 'Bike', 'Snow', 'Deer', 'End of the limits', 'Only right', 'Only left', 'Only straight', 'Only straight and right', \n",
    "               'Only straight and left', 'Take right', 'Take left', 'Circle crossroad', 'End of overtaking limit', 'End of overtaking limit for track']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./datasets/gtsrb-german-traffic-sign/Train.csv'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# derive the path to the training and testing CSV files\n",
    "\n",
    "path = './datasets/gtsrb-german-traffic-sign'  # 2019-1216 PP added\n",
    "\n",
    "trainPath = os.path.sep.join([path, \"Train.csv\"])\n",
    "testPath = os.path.sep.join([path, \"Test.csv\"])\n",
    "\n",
    "trainPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading training and testing data...\n",
      "[INFO] processed 4000 total images\n",
      "[INFO] processed 8000 total images\n",
      "[INFO] processed 12000 total images\n",
      "[INFO] processed 16000 total images\n",
      "[INFO] processed 20000 total images\n",
      "[INFO] processed 24000 total images\n",
      "[INFO] processed 28000 total images\n",
      "[INFO] processed 32000 total images\n",
      "[INFO] processed 36000 total images\n",
      "[INFO] processed 4000 total images\n",
      "[INFO] processed 8000 total images\n",
      "[INFO] processed 12000 total images\n"
     ]
    }
   ],
   "source": [
    "# load the training and testing data\n",
    "# 2019-1216 duurt lang!!\n",
    "\n",
    "print(\"[INFO] loading training and testing data...\")\n",
    "\n",
    "(trainX, trainY) = load_split(path, trainPath)\n",
    "(testX, testY) = load_split(path, testPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the images\n",
    "\n",
    "trainX = trainX.astype(\"float32\")/255.0\n",
    "testX = testX.astype(\"float32\")/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding of labels\n",
    "\n",
    "numLabels = len(np.unique(trainY))\n",
    "trainY = to_categorical(trainY, numLabels)\n",
    "testY = to_categorical(testY, numLabels)\n",
    "\n",
    "#take class weight into account\n",
    "classTotals = trainY.sum(axis = 0)\n",
    "classWeight = classTotals.max()/classTotals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.       , 185.70952  ],\n",
       "       [  1.0543405,  17.567118 ],\n",
       "       [  1.0551963,  17.33289  ],\n",
       "       [  1.0317469,  27.658865 ],\n",
       "       [  1.0475435,  19.696465 ],\n",
       "       [  1.0441779,  20.967205 ],\n",
       "       [  1.0054139,  92.85476  ],\n",
       "       [  1.0325664,  27.08264  ],\n",
       "       [  1.0317469,  27.658865 ],\n",
       "       [  1.0333872,  26.529932 ],\n",
       "       [  1.0483884,  19.402487 ],\n",
       "       [  1.0292962,  29.544697 ],\n",
       "       [  1.0509311,  18.570951 ],\n",
       "       [  1.052633 ,  18.055092 ],\n",
       "       [  1.0148325,  49.99872  ],\n",
       "       [  1.0108868,  61.903175 ],\n",
       "       [  1.0054139,  92.85476  ],\n",
       "       [  1.0236226,  35.134235 ],\n",
       "       [  1.0260465,  32.49917  ],\n",
       "       [  1.       , 185.70952  ],\n",
       "       [  1.0038611, 108.33056  ],\n",
       "       [  1.0030864, 118.17879  ],\n",
       "       [  1.0046369,  99.99744  ],\n",
       "       [  1.0077522,  76.46863  ],\n",
       "       [  1.0015409, 144.44073  ],\n",
       "       [  1.0342094,  25.999332 ],\n",
       "       [  1.0101013,  64.99834  ],\n",
       "       [  1.0007699, 162.49583  ],\n",
       "       [  1.008534 ,  72.22037  ],\n",
       "       [  1.0015409, 144.44073  ],\n",
       "       [  1.0061921,  86.664444 ],\n",
       "       [  1.0148325,  49.99872  ],\n",
       "       [  1.0007699, 162.49583  ],\n",
       "       [  1.0124351,  56.60232  ],\n",
       "       [  1.0054139,  92.85476  ],\n",
       "       [  1.0260465,  32.49917  ],\n",
       "       [  1.0046369,  99.99744  ],\n",
       "       [  1.       , 185.70952  ],\n",
       "       [  1.0500821,  18.840097 ],\n",
       "       [  1.0023131, 129.99667  ],\n",
       "       [  1.0038611, 108.33056  ],\n",
       "       [  1.0007699, 162.49583  ],\n",
       "       [  1.0007699, 162.49583  ]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct the image data augmentation generator\n",
    "# lees meer: \n",
    "#     https://www.pyimagesearch.com/2019/07/08/keras-imagedatagenerator-and-data-augmentation/\n",
    "#     dd. 8 juni 2019\n",
    "\n",
    "aug = ImageDataGenerator(\n",
    "    rotation_range = 10,\n",
    "    zoom_range = 0.15,\n",
    "    width_shift_range = 0.1,\n",
    "    height_shift_range = 0.2,\n",
    "    shear_range = 0.15,\n",
    "    horizontal_flip = False,\n",
    "    vertical_flip = False,\n",
    "    fill_mode = \"nearest\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "# initialize the optimizer and compile the model\n",
    "\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = Adam(lr = INIT_LR, decay = INIT_LR/(NUM_EPOCHS * 0.5))\n",
    "# 2019-1217 TEST: from tensorflow.keras.optimizers import SGD\n",
    "# 2019-1217 TEST: opt = SGD(lr=INIT_LR, momentum=0.9, decay=INIT_LR / NUM_EPOCHS * 0.5)\n",
    "\n",
    "model = objTSN.build(width = 32, height = 32, depth = 3, classes=numLabels)\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics = [\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (64, 43, 2) was passed for an output of shape (None, 2) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-739b8cf67576>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassWeight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[1;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[1;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[1;32m    975\u001b[0m                  outputs['metrics'])\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    251\u001b[0m   x, y, sample_weights = model._standardize_user_data(\n\u001b[1;32m    252\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m       extract_tensors_from_dataset=True)\n\u001b[0m\u001b[1;32m    254\u001b[0m   \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m   \u001b[0;31m# If `model._distribution_strategy` is True, then we are in a replica context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2536\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2537\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[0;32m-> 2538\u001b[0;31m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m   2539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2540\u001b[0m       \u001b[0;31m# If sample weight mode has not been set and weights are None for all the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    741\u001b[0m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[1;32m    742\u001b[0m                            \u001b[0;34m' was passed for an output of shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m                            \u001b[0;34m' while using as loss `'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m                            \u001b[0;34m'This loss expects targets to have the same shape '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                            'as the output.')\n",
      "\u001b[0;31mValueError\u001b[0m: A target array with shape (64, 43, 2) was passed for an output of shape (None, 2) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "#train the network\n",
    "\n",
    "H = model.fit_generator(\n",
    "    aug.flow(trainX, trainY, batch_size = BS),\n",
    "    validation_data = (testX, testY),\n",
    "    steps_per_epoch = trainX.shape[0]//BS,\n",
    "    epochs = NUM_EPOCHS,\n",
    "    class_weight = classWeight,\n",
    "    verbose = 1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate network\n",
    "predictions = model.predict(testX, batch_size = BS)\n",
    "print(classification_report(testY.argmax(axis = 1), predictions.argmax(axis = 1), target_names = labelNames))\n",
    "\n",
    "#save the model\n",
    "model.save(\"trainedmodel.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the training loss and accuracy\n",
    "\n",
    "N = np.arange(0, NUM_EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label = \"train_loss\")\n",
    "plt.plot(N, H.history[\"val_loss\"], label = \"val_loss\")\n",
    "plt.plot(N, H.history[\"accuracy\"], label = \"accuracy\")\n",
    "plt.plot(N, H.history[\"val_accuracy\"], label = \"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on DataSet\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc = \"lower left\")\n",
    "plt.savefig(\"plot.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
