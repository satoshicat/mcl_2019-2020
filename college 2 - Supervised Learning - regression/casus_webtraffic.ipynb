{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casus WebTraffic\n",
    "\n",
    "The case Webtraffic is based on following resources:\n",
    "- <a href=\"https://www.packtpub.com/big-data-and-business-intelligence/building-machine-learning-systems-python-second-edition\" target=\"_blank\">Building Machine Learning with Python (2e)</a>\n",
    "- <a href=\"https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-pyand \" target=\"_blank\">scikit-learn: Linear Regression Example</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An application of machine learning\n",
    "\n",
    "Let's get our hands dirty and take a look at our hypothetical web start-up, MLaaS, which sells the service of providing machine learning algorithms via HTTP. \n",
    "\n",
    "With increasing success of our company, the demand for better infrastructure increases to serve all incoming web requests successfully. We don't want to allocate too many resources as that would be too costly. On the other side, we will lose money, if we have not reserved enough resources to serve all incoming requests. \n",
    "\n",
    "Now, the question is, **when will we hit the limit of our current infrastructure, which we estimated to be at 100,000 requests per hour**. We would like to know in advance when we have to request additional servers in the cloud to serve all the incoming requests successfully without paying for unused ones.\n",
    "\n",
    "Lets enter the data science pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display plots in Notebook cell\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the data \n",
    "File `web_traffic.tsv` contains web statistics for the last month and aggregated in the file. \n",
    "\n",
    "The data is stored as the number of hits per hour. Each line contains the hour consecutively and the number of web hits in that hour\n",
    "\n",
    "**Activity**<br>\n",
    "Open the file `web_traffic.tsv` with a text-editor, and look at it briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "path = './datasets/'\n",
    "data = sp.genfromtxt(path + \"web_traffic.tsv\", delimiter=\"\\t\")\n",
    "# .tsv because it contains tab-separated values\n",
    "#help(sp.genfromtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display some data\n",
    "print (data[:10])\n",
    "\n",
    "# display the dimensions of the data...\n",
    "print('\\n', data.shape)\n",
    "\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have 743 data points with 2 dimensions.\n",
    "One caveat is still that we have some values in y that contain invalid values, <code>nan</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and cleaning the data\n",
    "\n",
    "Model linear regression in two dimensions, so we separate the dimensions into two vectors, each of size 743. The first vector, X, will contain the hours, and the other, y, will contain the Web hits in that particular hour. \n",
    "\n",
    "This splitting is done using the special index notation of SciPy, by which we can choose the columns individually.\n",
    "\n",
    "There are many more ways in which data can be selected from a SciPy array. \n",
    "Check out http://www.scipy.org/Tentative_NumPy_Tutorial for more details on indexing, slicing, and iterating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only one feature\n",
    "X = data[:, np.newaxis, 0]  # hours\n",
    "# type(X)\n",
    "print('Dimension feature X: {}'.format(X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target\n",
    "y = data[:, np.newaxis, 1]  # traffic\n",
    "# # type(y)\n",
    "print('Dimension target y: {}'.format(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y contains invalid values, <code>nan</code>. The question is what we can do with them?\n",
    "Let's check how many hours contain invalid data, by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup the data - how many NaN?\n",
    "import scipy as sp\n",
    "sp.sum(sp.isnan(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to do with the invalid data?\n",
    "As you can see, we are missing only 8 out of 743 entries, so we can afford to remove them.\n",
    "\n",
    "We can index a SciPy array with another array. <b>sp.isnan(y)</b> returns an array of Booleans indicating whether an entry is a number or not. Using <b>~</b>, we logically negate that array so that we choose only those elements from x and y where y contains valid numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup the data - remove NaN?\n",
    "X = X[~sp.isnan(y)]\n",
    "y = y[~sp.isnan(y)]\n",
    "\n",
    "# check there are no invalid numbers 'nan' anymore\n",
    "print('X contains {} NaN:'.format(sp.sum(sp.isnan(X))))\n",
    "print('y contains {} NaN:'.format(sp.sum(sp.isnan(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring data visually\n",
    "\n",
    "To get the first impression of our data, let's plot the data in a scatter plot using matplotlib. Matplotlib contains the pyplot package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display plots in Notebook cell\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the (X,y) points with dots of size 10\n",
    "plt.scatter(X, y, s=10)\n",
    "\n",
    "plt.title(\"Web traffic over the last month\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Hits/hour\")\n",
    "plt.xticks([w*7*24 for w in range(10)],\n",
    "               ['week %i' % w for w in range(10)])\n",
    "plt.autoscale(tight=True)\n",
    "# draw a slightly opaque, dashed grid\n",
    "plt.grid(True, linestyle='-', color='0.75')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the resulting chart, we can see that while in the first weeks the traffic stayed more or less the same, the last week shows a steep increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression model and learning algorithm\n",
    "Now that we have a first impression of the data, we return to the initial question: **How long will our server handle the incoming web traffic?**\n",
    "\n",
    "To answer this we have to do the following:\n",
    "\n",
    "1. Find the real model behind the noisy data points (**training**).\n",
    "2. Following this, use the model to extrapolate into the future to find the point in time where our infrastructure has to be extended (**prediction**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build our first model\n",
    "\n",
    "When we talk about models, you can think of them as simplified theoretical approximations of complex reality. As such there is always some inferiority involved, also called the **approximation error**. \n",
    "It is said to be a discrepancy between approximated value and exact value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training/testing sets\n",
    "X_train = X[:-20]\n",
    "X_test = X[-20:]\n",
    "# type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the targets into training/testing sets\n",
    "y_train = y[:-20]\n",
    "y_test = y[-20:]\n",
    "# type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\"\"\"\n",
    "# splitting arrays manually is very error-prune!\n",
    "X_train = X[:-20] # X[:-400\n",
    "X_test = X[-20:]  # X[-400:]\n",
    "y_train = y[:-20] # y[:-400]\n",
    "y_test = y[-20:]  # y[-400:]\n",
    "\"\"\"\n",
    "\n",
    "# a better split in training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping the array's for scikit-learn, since we only have a single feature\n",
    "X_train = X_train.reshape(-1, 1)\n",
    "y_train = y_train.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using the training sets\n",
    "regr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping the array's for scikit-learn, since we only have a single feature\n",
    "X_test = X_test.reshape(-1, 1)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The coefficient parameter (slope)\n",
    "print('Slope: ', regr.coef_)\n",
    "# The intercept parameter\n",
    "print('Intercept: ', regr.intercept_)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_pred))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = regr.score(X_test, y_test)\n",
    "print(\"Prediction Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data and the prediction line\n",
    "\n",
    "The prediction line is a simpel linear function with slope and intercept: `predict()` function.\n",
    "\n",
    "Slope and intercept are provided by the regression model `regr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope = regr.coef_[0][0]\n",
    "intercept = regr.intercept_[0]\n",
    "\n",
    "def predict(x):\n",
    "    return slope * x + intercept\n",
    "\n",
    "fitline = predict(X)  # X is feature!\n",
    "\n",
    "plt.title(\"Web traffic over the last month\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Hits/hour\")\n",
    "plt.xticks([w*7*24 for w in range(10)],\n",
    "               ['week %i' % w for w in range(10)])\n",
    "plt.autoscale(tight=True)\n",
    "# draw a slightly opaque, dashed grid\n",
    "plt.grid(True, linestyle='-', color='0.75')\n",
    "\n",
    "# dataset\n",
    "plt.scatter(X, y, s=10)\n",
    "plt.scatter(X_test, y_test, s=20)\n",
    "\n",
    "# prediction line\n",
    "plt.plot(X, fitline, color='red', lineWidth=4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative prediction line\n",
    "\n",
    "The prediction line is a polynomial of degree 1, slope and intercept is provided by the regression model `regr`, and the polynomial function is generated with help of SciPy's `polyd()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for plotting:\n",
    "\n",
    "# fm represents slope and intercept for a polynomial of degree 1 (f1).\n",
    "fm = np.array([regr.coef_[0][0], regr.intercept_[0]])\n",
    "print(\"Model parameters: {}\".format(fm)) # slope, intercept\n",
    "\n",
    "# generate polynomial function\n",
    "f1 = sp.poly1d(fm)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the (test/data) points with dots of size 10\n",
    "\n",
    "plt.title(\"Web traffic over the last month\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Hits/hour\")\n",
    "plt.xticks([w*7*24 for w in range(10)],\n",
    "               ['week %i' % w for w in range(10)])\n",
    "plt.autoscale(tight=True)\n",
    "# draw a slightly opaque, dashed grid\n",
    "plt.grid(True, linestyle='-', color='0.75')\n",
    "\n",
    "# draw dataset and prediction line\n",
    "plt.scatter(X, y, s=10)\n",
    "plt.scatter(X_test, y_test, s=20)\n",
    "\n",
    "# polynomial function f1()\n",
    "#plt.plot(X_test, f1(X_test), linewidth=4, color='red')\n",
    "plt.plot(X, f1(X), linewidth=4, color='red')\n",
    "\n",
    "# generate X-values for f1()\n",
    "# fx = sp.linspace(0, X[-1], 1000) \n",
    "# plt.plot(fx, f1(fx), linewidth=4, color='blue')\n",
    "\n",
    "plt.legend([\"d=%i\" % f1.order], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering our initial question\n",
    "\n",
    "Finally we have arrived at a model which we think represents the underlying process; it is now a simple task of finding out when our infrastructure will reach 100,000 requests per hour. We have to calculate when our model function reaches the value 100,000.\n",
    "\n",
    "SciPy's `optimize` module has the function `fsolve` that achieves this, when providing an initial starting position with parameter `x0`.\n",
    "* Having a polynomial of degree 1, we could simply compute the inverse of the function and calculate its value at 100,000. Of course, we would like to have an approach that is applicable to any model function easily.\n",
    "\n",
    "* This can be done by subtracting 100,000 from the polynomial, which results in another polynomial, and finding its root. \n",
    "\n",
    "\n",
    "As every entry in our input data file corresponds to one hour, and we have 743 of them, we set the starting position to some value after that. Let `f1` be the polynomial of degree 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the polynomial function\n",
    "\n",
    "print(\"f1(x)= {}\\n\".format(f1))\n",
    "print(\"f1(x)-100,000= {}\\n\".format((f1-100000)))\n",
    "\n",
    "# calculate the week of the 100000 hits/hour ...\n",
    "\n",
    "from scipy.optimize import fsolve\n",
    "reached_max = fsolve(f1-100000, x0=800)/(7*24)\n",
    "print(\"100,000 hits/hour expected at week {0:4.0f}\".format(reached_max[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial regression\n",
    "using higher degree polynomials\n",
    "\n",
    "## Optional reading.\n",
    "\n",
    "Using SciPy's `polyfit()` method to provide higher degree polynomials than the linear model: see Jupyter notebook `casus_webtraffic_scipy.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**: 100,000 hits/hour expected at week Â±9.9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "You just learned two important things, of which the most important one is that as a typical machine learning operator, you will spend most of your time in understanding and refining the data.\n",
    "\n",
    "And we hope that this example helped you to  start switching your mental focus from algorithms to data. Then you learned how important it is to have the correct experiment setup and that it is vital to not mix up training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
