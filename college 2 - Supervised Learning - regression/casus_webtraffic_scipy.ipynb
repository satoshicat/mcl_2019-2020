{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "\n",
    "<b>Bron</b> Notebook <a href=\"https://www.packtpub.com/big-data-and-business-intelligence/building-machine-learning-systems-python-second-edition\" target=\"_blank\">Building Machine Learning with Python (2e)</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (tiny) application of machine learning\n",
    "\n",
    "Let's get our hands dirty and take a look at our hypothetical web start-up, MLaaS, which sells the service of providing machine learning algorithms via HTTP. \n",
    "\n",
    "With increasing success of our company, the demand for better infrastructure increases to serve all incoming web requests successfully. We don't want to allocate too many resources as that would be too costly. On the other side, we will lose money, if we have not reserved enough resources to serve all incoming requests. \n",
    "\n",
    "Now, the question is, <b>when will we hit the limit of our current infrastructure, which we estimated to be at 100,000 requests per hour</b>. We would like to know in advance when we have to request additional servers in the cloud to serve all the incoming requests successfully without paying for unused ones.\n",
    "\n",
    "Lets enter the data science pipeline.<br>\n",
    "<b>?:</b> Which activities are included in the data scene pipeline?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reading in the data \n",
    "File `web_traffic.tsv` contains web statistics for the last month and aggregated in the file. \n",
    "\n",
    "The data is stored as the number of hits per hour. Each line contains the hour consecutively and the number of web hits in that hour\n",
    "\n",
    "**activity**<br>\n",
    "Open the file `web_traffic.tsv` with a text-editor, and look at it briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web stats for the last month and aggregated them in web_traffic.tsv \n",
    "# Using SciPy's genfromtxt()\n",
    "# Investigate: what is the type of the read data object?\n",
    "\n",
    "import scipy as sp\n",
    "path = 'datasets/'\n",
    "data = sp.genfromtxt(path + \"web_traffic.tsv\", delimiter=\"\\t\")\n",
    "# .tsv because it contains tab-separated values\n",
    "#help(sp.genfromtxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display some data\n",
    "print (data[:10])\n",
    "\n",
    "# display the dimensions of the data...\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have 743 data points with two dimensions.\n",
    "One caveat is still that we have some values in y that contain invalid values, <code>nan</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing and cleaning the data\n",
    "\n",
    "It is more convenient for SciPy to separate the dimensions into two vectors, each of size 743. The first vector, x, will contain the hours, and the other, y, will contain the Web hits in that particular hour. This splitting is done using the special index notation of SciPy, by which we can choose the columns individually.\n",
    "\n",
    "There are many more ways in which data can be selected from a SciPy array. \n",
    "Check out http://www.scipy.org/Tentative_NumPy_Tutorial for more details on indexing, slicing, and iterating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[:,0]\n",
    "y = data[:,1]\n",
    "\n",
    "# how do I print the first 10 elements of x and y?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One caveat is still that we have some values in y that contain invalid values, <code>nan</code>. The question is what we can do with them? \n",
    "\n",
    "Let's check how many hours contain invalid data, by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.sum(sp.isnan(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what to do with the invalid data?\n",
    "As you can see, we are missing only 8 out of 743 entries, so we can afford to remove them.\n",
    "\n",
    "We can index a SciPy array with another array. <b>sp.isnan(y)</b> returns an array of Booleans indicating whether an entry is a number or not. Using <b>~</b>, we logically negate that array so that we choose only those elements from x and y where y contains valid numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sp.isnan(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x[~sp.isnan(y)]\n",
    "y = y[~sp.isnan(y)]\n",
    "\n",
    "# ?: how to check there are no invalid numbers 'nan' anymore?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exploring data visually\n",
    "\n",
    "To get the first impression of our data, let's plot the data in a scatter plot using matplotlib. Matplotlib contains the pyplot package.\n",
    "\n",
    "<b>?:</b> Which (Matplotlib) style represents the pyplot package: MATLAB- or Object Oriented-style?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display plots in Notebook cell\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the (x,y) points with dots of size 10\n",
    "plt.scatter(x, y, s=10)\n",
    "\n",
    "plt.title(\"Web traffic over the last month\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Hits/hour\")\n",
    "plt.xticks([w*7*24 for w in range(10)],\n",
    "               ['week %i' % w for w in range(10)])\n",
    "plt.autoscale(tight=True)\n",
    "# draw a slightly opaque, dashed grid\n",
    "plt.grid(True, linestyle='-', color='0.75')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the resulting chart, we can see that while in the first weeks the traffic stayed more or less the same, the last week shows a steep increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the right model and learning algorithm\n",
    "Now that we have a first impression of the data, we return to the initial question: <b>How long will our server handle the incoming web traffic? </b>\n",
    "\n",
    "To answer this we have to do the following:\n",
    "\n",
    "1. Find the real model behind the noisy data points.\n",
    "2. Following this, use the model to extrapolate into the future to find the point in time where our infrastructure has to be extended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build our first model\n",
    "\n",
    "#### Before building our first model\n",
    "\n",
    "When we talk about models, you can think of them as simplified theoretical approximations of complex reality. As such there is always some inferiority involved, also called the <b>approximation error</b>. \n",
    "It is said to be a discrepancy between approximated value and exact value.\n",
    "\n",
    "This error will guide us in choosing the right model among the myriad of choices we have. And <b>this error will be calculated as the squared distance of the model's prediction <code>f(x)</code> to the real data <code>x</code></b>. For example, for a learned model function f, the error is calculated as follows:\n",
    "        \n",
    "        error = sum((f(x) - y)**2)\n",
    "\n",
    "The vectors x and y contain the web stats data that we have extracted earlier. It is the beauty of SciPy's vectorized functions that we exploit here with f(x). \n",
    "\n",
    "The trained model is assumed to take a vector and return the results again as a vector of the same size so that we can use it to calculate the difference to y.\n",
    "\n",
    "See more about approximation error: https://en.wikipedia.org/wiki/Approximation_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(f, x, y):\n",
    "    return sp.sum((f(x)-y)**2)\n",
    "\n",
    "# note that a function is treated as a parameter in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### starting with a simple straight line\n",
    "\n",
    "Let's assume for a second that the underlying model is a straight line. Then the challenge is how to best put that line into the chart so that it results in the smallest approximation error. \n",
    "\n",
    "straight line: <code>f(x) = a * x + b</code>, a and b to be determined\n",
    "\n",
    "SciPy's <code>polyfit()</code> function does exactly that. Given data <code>x</code> and <code>y</code> and the desired order of the polynomial (a straight line has order 1), it finds the model function that minimizes the error function defined earlier.\n",
    "\n",
    "The <code>polyfit()</code> function returns the parameters of the fitted model function, <code>fp1</code>. And by setting <code>full=True</code>, we also get additional background information on the fitting process. Of this, only residuals are of interest, which is exactly the error of the approximation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp1, residuals, rank, sv, rcond = sp.polyfit(x, y, 1, full=True)\n",
    "\n",
    "print(\"Model parameters: %s\" % fp1) # slope, intercept\n",
    "# -> Model parameters: [2.59619213  989.02487106]\n",
    "\n",
    "print(\"residuals: \", residuals)\n",
    "# -> [  3.17389767e+08]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means the best straight line fit is the following function:\n",
    "\n",
    "            f(x) = 2.59619213 * x + 989.02487106\n",
    "\n",
    "We then use <code>poly1d()</code> to create a model function from the model parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = sp.poly1d(fp1)\n",
    "print(error(f1, x, y))    # 317389767.34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use <code>f1()</code> to plot our first trained model. \n",
    "\n",
    "In addition to the preceding plotting instructions, we simply add the following code. This will produce the following plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = sp.poly1d(fp1)\n",
    "\n",
    "# plot the (x,y) points with dots of size 10\n",
    "plt.scatter(x, y, s=10)\n",
    "\n",
    "plt.title(\"Web traffic over the last month\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Hits/hour\")\n",
    "plt.xticks([w*7*24 for w in range(10)],\n",
    "               ['week %i' % w for w in range(10)])\n",
    "plt.autoscale(tight=True)\n",
    "# draw a slightly opaque, dashed grid\n",
    "plt.grid(True, linestyle='-', color='0.75')\n",
    "\n",
    "# draw the model function: straight line f1(x)\n",
    "fx = sp.linspace(0,x[-1], 1000) # generate X-values for f1()\n",
    "plt.plot(fx, f1(fx), linewidth=4, color='red')\n",
    "plt.legend([\"d=%i\" % f1.order], loc=\"upper left\")\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the first 4 weeks are not that far off, although we clearly see that there is something wrong with our initial assumption that the underlying model is a straight line. And then, how good or how bad actually is the error of 317,389,767.34?\n",
    "\n",
    "Although our first model clearly is not the one we would use, it serves a very important purpose in the workflow. We will use it as our baseline until we find a better one. Whatever model we come up with in the future, we will compare it against the current baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### towards some advanced stuff\n",
    "\n",
    "Let's now fit a more complex model, a polynomial of degree 2, to see whether it better understands our data:\n",
    "\n",
    "      f(x) = a * x**2  + b * x + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp2, res2, rank2, sv2, rcond2 = sp.polyfit(x, y, 2, full=True)\n",
    "print(fp2)  # array([1.05322215e-02, -5.26545650e+00, 1.97476082e+03])\n",
    "\n",
    "f2 = sp.poly1d(fp2)\n",
    "print(error(f2, x, y))   # 179983507.878"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error is 179,983,507.878, which is almost half the error of the straight line model. This is good but unfortunately this comes with a price: We now have a more complex function, meaning that we have one parameter more to tune inside <code>polyfit()</code>. The fitted polynomial is as follows:\n",
    "\n",
    "      f(x) = 0.0105322215 * x**2  - 5.26545650 * x + 1974.76082\n",
    "\n",
    "You will get the following plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the (x,y) points with dots of size 10\n",
    "plt.scatter(x, y, s=10)\n",
    "\n",
    "plt.title(\"Web traffic over the last month\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Hits/hour\")\n",
    "plt.xticks([w*7*24 for w in range(10)],\n",
    "               ['week %i' % w for w in range(10)])\n",
    "plt.autoscale(tight=True)\n",
    "# draw a slightly opaque, dashed grid\n",
    "plt.grid(True, linestyle='-', color='0.75')\n",
    "\n",
    "# draw the model function: f2(x)\n",
    "fx = sp.linspace(0,x[-1], 1000) # generate X-values for plotting\n",
    "plt.plot(fx, f2(fx), linewidth=4, color='red')\n",
    "plt.legend([\"d=%i\" % f2.order], loc=\"upper left\")\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if more complexity gives better results, why not increase the complexity even more? Let's try it for degrees 3, 10, and 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help function to draw the plot with a model function, \n",
    "#      and to save the plot in a file\n",
    "\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# all examples will have three classes in this file\n",
    "colors = ['g', 'k', 'b', 'm', 'r']\n",
    "linestyles = ['-', '-.', '--', ':', '-']\n",
    "\n",
    "# plot input data...\n",
    "def plot_models(x, y, models, fname, mx=None, ymax=None, xmin=None):\n",
    "\n",
    "    plt.figure(num=None, figsize=(8, 6))\n",
    "    plt.clf()\n",
    "    plt.scatter(x, y, s=10)\n",
    "    plt.title(\"Web traffic over the last month\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Hits/hour\")\n",
    "    plt.xticks(\n",
    "        [w * 7 * 24 for w in range(10)], ['week %i' % w for w in range(10)])\n",
    "\n",
    "    if models:\n",
    "        if mx is None:\n",
    "            mx = sp.linspace(0, x[-1], 1000) # all data-points\n",
    "        for model, style, color in zip(models, linestyles, colors):\n",
    "            # print \"Model:\",model\n",
    "            # print \"Coeffs:\",model.coeffs\n",
    "            plt.plot(mx, model(mx), linestyle=style, linewidth=2, c=color)\n",
    "\n",
    "        plt.legend([\"d=%i\" % m.order for m in models], loc=\"upper left\")\n",
    "\n",
    "    plt.autoscale(tight=True)\n",
    "    plt.ylim(ymin=0)\n",
    "    if ymax:\n",
    "        plt.ylim(ymax=ymax)\n",
    "    if xmin:\n",
    "        plt.xlim(xmin=xmin)\n",
    "\n",
    "    # draw a slightly opaque, dashed grid\n",
    "    plt.grid(True, linestyle='-', color='0.75')\n",
    "    \n",
    "    # save plot in an image file fname\n",
    "    if fName is not None:  #2017_1214 added\n",
    "        plt.savefig(fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# polynomial degree 3, 10 and 100:\n",
    "f3 = sp.poly1d(sp.polyfit(x, y, 3))\n",
    "f10 = sp.poly1d(sp.polyfit(x, y, 10))\n",
    "f100 = sp.poly1d(sp.polyfit(x, y, 100))\n",
    "\n",
    "print(\"Error d=3  : \", error(f3, x, y))   # 139,350,144.031725\n",
    "print(\"Error d=10 : \", error(f10, x, y))  # 121,942,326.363461\n",
    "print(\"Error d=100: \", error(f100, x, y)) # 109,318,004.475556\n",
    "\n",
    "# subfolder, must exists before running the code\n",
    "# 2016_1218 Peter added\n",
    "CHART_DIR = \"figures\" \n",
    "\n",
    "# plot the models f1 thru f100 in one figure\n",
    "fName = os.path.join(CHART_DIR, \"figure_all_models.png\")\n",
    "plot_models( x, y, [f1, f2, f3, f10, f100], fName )\n",
    "\n",
    "# show in notebook\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, we do see <code>d=53</code> for the polynomial that had been fitted with 100 degrees. \n",
    "\n",
    "Instead, we see warnings on the console:\n",
    "\n",
    "       Polyfit may be poorly conditioned\n",
    "\n",
    "This means because of numerical errors, <code>polyfit</code> cannot determine a good fit with 100 degrees. Instead, it figured that 53 must be good enough.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "However, taking a closer look at the fitted curves, we start to wonder whether they also capture the true process that generated that data. Framed differently, <b>do our models correctly represent the underlying mass behavior of customers visiting our website</b>?\n",
    "\n",
    "Looking at the polynomial of degree 10 and 53, we see wildly oscillating behavior. It seems that the <b>models are fitted too much to the data. So much that it is now capturing not only the underlying process but also the noise</b>. This is called <b>overfitting</b>.\n",
    "\n",
    "At this point, we have the following choices:\n",
    "\n",
    "    • Choosing one of the fitted polynomial models.\n",
    "    • Switching to another more complex model class.\n",
    "    • Thinking differently about the data and start again.\n",
    "    \n",
    "Out of the five fitted models, the first order model clearly is too simple, and the models of order 10 and 53 are clearly overfitting. \n",
    "\n",
    "Only the second and third order models seem to somehow match the data. However, if we extrapolate them at both borders, we see them going berserk.\n",
    "\n",
    "<b>Switching to a more complex class seems also not to be the right way to go</b>. What arguments would back which class? At this point, we realize that we probably have not fully understood our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepping back to go forward – another look at our data\n",
    "\n",
    "So, we step back and take another look at the data. It seems that there is an inflection point between weeks 3 and 4. So let's separate the data and train two lines using week 3.5 as a separation point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# calculate the inflection point in hours (must be integer)\n",
    "inflection = math.floor(3.5*7*24) \n",
    "\n",
    "xa = x[:inflection] # data before the inflection point\n",
    "ya = y[:inflection]\n",
    "xb = x[inflection:] # data after the inflection point\n",
    "yb = y[inflection:]\n",
    "\n",
    "fa = sp.poly1d(sp.polyfit(xa, ya, 1))\n",
    "fb = sp.poly1d(sp.polyfit(xb, yb, 1))\n",
    "\n",
    "fa_error = error(fa, xa, ya)\n",
    "fb_error = error(fb, xb, yb)\n",
    "print(\"Error inflection=%f\" % (fa_error + fb_error))\n",
    "# -> Error inflection=132950348.197616"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# fit and plot a model using the knowledge about inflection point\n",
    "# calculate the inflection point in hours\n",
    "inflection = math.floor(3.5*7*24)\n",
    "\n",
    "# data before the inflection point\n",
    "xa = x[:inflection] \n",
    "ya = y[:inflection]\n",
    "\n",
    "# data after the inflection point\n",
    "xb = x[inflection:] \n",
    "yb = y[inflection:]\n",
    "\n",
    "# calculate the new models fa and fb\n",
    "fa = sp.poly1d(sp.polyfit(xa, ya, 1))\n",
    "fb = sp.poly1d(sp.polyfit(xb, yb, 1))\n",
    "\n",
    "fa_error = error(fa, xa, ya)\n",
    "fb_error = error(fb, xb, yb)\n",
    "\n",
    "print(\"Error inflection=%f\" % (fa_error + fb_error))   \n",
    "                        # Error inflection=132950348.197616\n",
    "\n",
    "# plot the models\n",
    "plot_models(x, y, [fa, fb], os.path.join(CHART_DIR, \"figure_inflection.png\"))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the first line, we train with the data up to week 3, and in the second line we train with the remaining data.\n",
    "\n",
    "Clearly, the combination of these two lines seems to be a much better fit to the data than anything we have modeled before. \n",
    "\n",
    "But still, the combined error is higher than the higher order polynomials. Can we trust the error at the end?\n",
    "\n",
    "Asked differently, why do we trust the straight line fitted only at the last week of our data more than any of the more complex models? \n",
    "\n",
    "It is because we assume that it will capture future data better. If we plot the models into the future, we see how right we are (d=1 is again our initial straight line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extrapolating into the future\n",
    "# linespace: calculation x in hours from week 0 until week 6\n",
    "plot_models(\n",
    "    x, y, [f1, f2, f3, f10, f100],\n",
    "    os.path.join(CHART_DIR, \"figure_06.png\"),\n",
    "    mx=sp.linspace(0 * 7 * 24, 6 * 7 * 24, 100),\n",
    "                   ymax=10000, xmin=0 * 7 * 24)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models of degree 10 and 53 don't seem to expect a bright future of our start-up. They tried so hard to model the given data correctly that they are clearly useless to extrapolate beyond. This is called <b>overfitting</b>.\n",
    "\n",
    "On the other hand, the lower degree models seem not to be capable of capturing the data good enough. This is called <b>underfitting</b>.\n",
    "\n",
    "So let's play fair to <b>models of degree 2 and above and try out how they behave if we fit them only to the data of the last week.</b>\n",
    "\n",
    "After all, <b>we believe that the last week says more about the future than the data prior to it</b>. The result can be seen in the following psychedelic chart, which further shows how badly the problem of overfitting is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Trained only on data after inflection point\")\n",
    "fb1 = fb\n",
    "fb2 = sp.poly1d(sp.polyfit(xb, yb, 2))\n",
    "fb3 = sp.poly1d(sp.polyfit(xb, yb, 3))\n",
    "fb10 = sp.poly1d(sp.polyfit(xb, yb, 10))\n",
    "fb100 = sp.poly1d(sp.polyfit(xb, yb, 100))\n",
    "\n",
    "# plot of the models on data after inflection point...\n",
    "plot_models(\n",
    "    x, y, [fb1, fb2, fb3, fb10, fb100],\n",
    "    os.path.join(CHART_DIR, \"figure_07.png\"),\n",
    "    mx=sp.linspace(0 * 7 * 24, 6 * 7 * 24, 100),\n",
    "    ymax=10000, xmin=0 * 7 * 24)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, judging from the errors of the models when trained only on the data from week 3.5 and later, we still should choose the most complex one (note that we also calculate the error only on the time after the inflection point):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nErrors for only the time after inflection point\")\n",
    "for f in [fb1, fb2, fb3, fb10, fb100]:\n",
    "    print(\"Error d=%i: %f\" % (f.order, error(f, xb, yb)))\n",
    "\n",
    "print(\"\\nError inflection=%f\" % (error(fa, xa, ya) + error(fb, xb, yb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing\n",
    "\n",
    "If we only had some data from the future that we could use to measure our models against, then we should be able to judge our model choice only on the resulting approximation error.\n",
    "\n",
    "Although we cannot look into the future, we can and should simulate a similar effect by holding out a part of our data. Let's remove, for instance, a <b>certain percentage of the data and train on the remaining one</b>. \n",
    "\n",
    "Then we used the held-out data to calculate the error. As the model has been trained not knowing the held-out data, we should get a more realistic picture of how the model will behave in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating training from testing data\n",
    "frac = 0.3\n",
    "split_idx = int(frac * len(xb))\n",
    "shuffled = sp.random.permutation(list(range(len(xb))))\n",
    "test = sorted(shuffled[:split_idx])\n",
    "train = sorted(shuffled[split_idx:])\n",
    "\n",
    "# models trained only on the time after inflection point\n",
    "fbt1 = sp.poly1d(sp.polyfit(xb[train], yb[train], 1))\n",
    "fbt2 = sp.poly1d(sp.polyfit(xb[train], yb[train], 2))\n",
    "\n",
    "# print the polynomial function parameters ...\n",
    "print(\"fbt2(x)= \\n%s\"%fbt2)    \n",
    "print(\"\\nfbt2(x)-100,000= \\n%s\\n\"%(fbt2-100000))\n",
    "\n",
    "# generate model functions ...\n",
    "fbt3 = sp.poly1d(sp.polyfit(xb[train], yb[train], 3))\n",
    "fbt10 = sp.poly1d(sp.polyfit(xb[train], yb[train], 10))\n",
    "fbt100 = sp.poly1d(sp.polyfit(xb[train], yb[train], 100))\n",
    "\n",
    "# print the approximation errors ...\n",
    "print(\"\\nTest errors for only the time after inflection point\")\n",
    "for f in [fbt1, fbt2, fbt3, fbt10, fbt100]:\n",
    "    print(\"Error d=%i: \\t%f\" % (f.order, error(f, xb[test], yb[test])))\n",
    "\n",
    "# plot the models ...\n",
    "plot_models(\n",
    "    x, y, [fbt1, fbt2, fbt3, fbt10, fbt100],\n",
    "    os.path.join(CHART_DIR, \"figure_08.png\"),\n",
    "    mx=sp.linspace(0 * 7 * 24, 6 * 7 * 24, 100),\n",
    "    ymax=10000, xmin=0 * 7 * 24)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that we finally have a <em>clear</em> winner (<em>see Note</em>): The model with degree 2 has the lowest test error, which is the error when measured using data that the model did not see during training. And this gives us hope that we won't get bad surprises when future data arrives.\n",
    "\n",
    "<em>Note</em>: the training set are random numbers! It is not always clear that d=2 had lowest error. Running after several samples, d=2 had <b>mostly</b> the lowest error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering our initial question\n",
    "\n",
    "Finally we have arrived at a model which we think represents the underlying process best; it is now a simple task of finding out when our infrastructure will reach 100,000 \n",
    "requests per hour. We have to calculate when our model function reaches the value 100,000.\n",
    "\n",
    "Having a polynomial of degree 2, we could simply compute the inverse of the function and calculate its value at 100,000. Of course, we would like to have an approach that is applicable to any model function easily.\n",
    "\n",
    "This can be done by subtracting 100,000 from the polynomial, which results in another polynomial, and finding its root. \n",
    "\n",
    "SciPy's <code>optimize</code> module has the function <code>fsolve</code> that achieves this, when providing an initial starting position with parameter <code>x0</code>. \n",
    "\n",
    "As every entry in our input data file corresponds to one hour, and we have 743 of them, we set the starting position to some value after that. Let <code>fbt2</code> be the winning polynomial of degree 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbt2 = sp.poly1d(sp.polyfit(xb[train], yb[train], 2))\n",
    "\n",
    "# pint the polynomial model function\n",
    "print(\"fbt2(x)= \\n%s\\n\" % fbt2)\n",
    "print(\"fbt2(x)-100,000= \\n%s\\n\\n\" % (fbt2-100000))\n",
    "\n",
    "# calculate the week of the 100000 hits/hour ...\n",
    "from scipy.optimize import fsolve\n",
    "reached_max = fsolve(fbt2-100000, x0=800)/(7*24)\n",
    "print(\"100,000 hits/hour expected at week %f\" % reached_max[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... so our model tells us that, given the current user behavior and traction of our start-up, it will take another month until we have reached our capacity threshold.\n",
    "\n",
    "Of course, there is a certain uncertainty involved with our prediction. To get a real picture of it, one could draw in more sophisticated statistics to find out about the variance we have to expect when looking farther and farther into the future.\n",
    "\n",
    "And then there are the user and underlying user behavior dynamics that we cannot model accurately. However, at this point, we are fine with the current predictions. After all, we can prepare all time-consuming actions now. If we then monitor our web traffic closely, we will see in time when we have to allocate new resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Congratulations! You just learned two important things, of which the most important one is that as a typical machine learning operator, <b>you will spend most of your time in understanding and refining the data </b>—exactly what we just did in our first tiny machine learning example.\n",
    "\n",
    "And we hope that this example helped you to  <b>start switching your mental focus from algorithms to data</b>. Then you learned how important it is to have the correct experiment setup and that it is vital to not mix up training and testing.\n",
    "\n",
    "Admittedly, the use of polynomial fitting is not the coolest thing in the machine learning world. We have chosen it to not distract you by the coolness of some shiny algorithm when we conveyed the two most important messages we just summarized earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
